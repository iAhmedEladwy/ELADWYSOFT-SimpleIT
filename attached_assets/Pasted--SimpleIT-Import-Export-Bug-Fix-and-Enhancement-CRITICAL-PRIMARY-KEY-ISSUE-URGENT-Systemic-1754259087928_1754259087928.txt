# SimpleIT Import/Export Bug Fix and Enhancement - CRITICAL PRIMARY KEY ISSUE

## URGENT: Systemic Primary Key Generation Failure

**Critical Pattern Identified:**
- Assets import: `null value in column "asset_id" of relation "assets" violates not-null constraint`
- Tickets import: `null value in column "ticket_id" of relation "tickets" violates not-null constraint`
- **Result**: 0 records imported across ALL entity types

**Root Cause:** The import system is failing to generate or assign primary key values for ANY entity type before database insertion.

## Immediate Priority Fixes Required

### 1. Fix Primary Key Generation Across All Entities

**Affected Tables/Entities:**
- `assets` table → `asset_id` column
- `tickets` table → `ticket_id` column  
- `employees` table → likely `employee_id` column
- Any other entities with similar patterns

**Universal Primary Key Fix:**
```javascript
// Generic primary key handler for all entities
const generatePrimaryKey = (entityType) => {
  switch(entityType) {
    case 'assets':
      return generateAssetId(); // AST-000001, UUID, or auto-increment
    case 'tickets':
      return generateTicketId(); // TKT-000001, UUID, or auto-increment
    case 'employees':
      return generateEmployeeId(); // EMP-000001, UUID, or auto-increment
    default:
      return crypto.randomUUID(); // Fallback to UUID
  }
};

// Updated import process for ANY entity
const importEntityRecord = async (entityType, rowData, rowIndex) => {
  try {
    const processedData = { ...rowData };
    const primaryKeyField = getPrimaryKeyField(entityType); // asset_id, ticket_id, etc.
    
    // Method 1: Auto-increment (recommended) - exclude from INSERT
    if (isAutoIncrement(entityType, primaryKeyField)) {
      delete processedData[primaryKeyField];
      const query = buildInsertQuery(entityType, processedData, true); // true = return ID
      const result = await db.query(query, Object.values(processedData));
      return { success: true, id: result.rows[0][primaryKeyField] };
    }
    
    // Method 2: Application-generated ID
    else {
      processedData[primaryKeyField] = generatePrimaryKey(entityType);
      const query = buildInsertQuery(entityType, processedData, false);
      await db.query(query, Object.values(processedData));
      return { success: true, id: processedData[primaryKeyField] };
    }
    
  } catch (error) {
    return { 
      success: false, 
      error: error.message,
      row: rowIndex + 1,
      entity: entityType
    };
  }
};
```

### 2. Database Schema Investigation Required

**Immediate Actions:**
1. **Check database schema** for all import-enabled tables:
   ```sql
   -- Check if primary keys are auto-increment/serial
   SELECT table_name, column_name, column_default, is_nullable 
   FROM information_schema.columns 
   WHERE table_name IN ('assets', 'tickets', 'employees') 
   AND column_name LIKE '%_id';
   
   -- Check for sequences (PostgreSQL)
   SELECT schemaname, sequencename, last_value 
   FROM pg_sequences 
   WHERE sequencename LIKE '%_id_seq';
   ```

2. **Verify primary key configuration:**
   - Are they `SERIAL`/`AUTO_INCREMENT`? → Don't include in INSERT
   - Are they manually assigned? → Generate in application
   - Are they UUIDs? → Generate UUID before INSERT

### 3. Fix the Fetch API Error (Secondary Priority)
- **Locate the problematic fetch call** in the import functionality
- **Check all fetch calls** in the import/export modules  
- **Ensure HTTP methods are strings**, not objects:
  ```javascript
  // WRONG - method is an object
  fetch(url, { method: requestConfig, ... })
  
  // CORRECT - method is a string  
  fetch(url, { method: 'POST', ... })
  ```
- **Add comprehensive file format validation** for:
  - CSV files (.csv)
  - Excel files (.xlsx, .xls)
  - JSON files (.json)
  - TSV files (.tsv)
- **Create proper error messages** for unsupported formats instead of generic failures
- **Add file extension and MIME type checking**

### 3. Implement Field Mapping Confirmation System

Create a **two-step import process**:

#### Step 1: File Analysis and Preview
- Parse the uploaded file and extract column headers
- Display first 3-5 rows as preview
- Show detected file format and encoding
- **Identify entity type** (assets, tickets, employees) and corresponding primary key requirements
- Allow user to proceed to mapping step

#### Step 2: Field Mapping Interface
Create an intuitive mapping interface with:

**Visual Mapping Component:**
- Left column: Source file headers (from uploaded file)
- Right column: Target database fields (SimpleIT entity fields)
- **Primary key field handling**: Clearly show auto-generated fields that should NOT be mapped
- Drag-and-drop or dropdown selection for mapping
- Clear visual indicators for:
  - ✅ Mapped fields
  - ⚠️ Unmapped required fields
  - ❓ Optional fields
  - 🚫 Auto-generated fields (asset_id, ticket_id, etc.)
  - 🔄 Data type mismatches

**Field Mapping Features:**
- **Required field validation**: Highlight mandatory fields that must be mapped
- **Auto-generated field handling**: 
  - Clearly indicate fields like `asset_id`, `ticket_id` that shouldn't be mapped from source data
  - Show these fields as "System Generated" with green checkmarks
  - Prevent users from trying to map to these fields
- **Data type preview**: Show sample data for each column with validation status
- **Smart suggestions**: Auto-suggest mappings based on column names
- **Custom field mapping**: Allow mapping to custom entity fields
- **Data transformation options**: 
  - Date format conversion (show detected format vs required format)
  - Enum value mapping (show invalid values and suggested corrections)
  - Text cleaning (trim whitespace, case conversion)
  - Default value assignment for empty/invalid fields
- **Real-time validation**: Show validation errors as user maps fields
- **Primary key handling**: Clearly show which fields are auto-generated vs required from source

**Validation and Confirmation:**
- Show mapping summary with data validation results before import
- **Primary key generation preview**: Show that IDs will be auto-generated
- Display potential data issues, invalid enum values, and date format problems
- Provide "Test Import" with first 5 records showing:
  - How primary keys will be generated
  - Validation results for all mapped fields
  - Preview of final database records
- Allow user to download mapping template for future use
- Save mapping configurations for reuse
- Show statistics: "X valid records, Y records with warnings, Z records will be skipped"
- **Critical validation**: Ensure no primary key fields are mapped from source data

### 4. Critical Database Constraint Issues to Fix

**URGENT: Primary Key Generation Problem**
- **Current Issue**: `null value in column "asset_id" of relation "assets" violates not-null constraint`
- **Result**: 0 records imported successfully, all rows failing

**Root Cause Analysis:**
The import process is not generating or assigning `asset_id` values before database insertion.

**Required Fixes:**

#### A. Asset ID Generation
```javascript
// Ensure asset_id is generated for each record before database insertion
const generateAssetId = () => {
  // Option 1: Use database auto-increment (preferred)
  // Remove asset_id from INSERT statement, let database generate it
  
  // Option 2: Generate UUID if using UUID primary keys
  return crypto.randomUUID();
  
  // Option 3: Generate sequential ID based on existing records
  const lastAsset = await getLastAssetId();
  return `AST-${String(lastAsset + 1).padStart(6, '0')}`;
};
```

#### B. Import Process Fix
```javascript
// WRONG - Current approach (causing the error)
const importRecord = async (rowData) => {
  await db.query(
    'INSERT INTO assets (asset_id, name, type, ...) VALUES ($1, $2, $3, ...)',
    [null, rowData.name, rowData.type, ...] // asset_id is null!
  );
};

// CORRECT - Fixed approach
const importRecord = async (rowData) => {
  // Option 1: Let database auto-generate asset_id
  await db.query(
    'INSERT INTO assets (name, type, employee_id, ...) VALUES ($1, $2, $3, ...) RETURNING asset_id',
    [rowData.name, rowData.type, rowData.employee_id, ...]
  );
  
  // Option 2: Generate asset_id in application
  const assetId = generateAssetId();
  await db.query(
    'INSERT INTO assets (asset_id, name, type, ...) VALUES ($1, $2, $3, ...)',
    [assetId, rowData.name, rowData.type, ...]
  );
};
```

#### C. Database Schema Validation
Check if `asset_id` column should be:
- **AUTO_INCREMENT/SERIAL**: Let database generate automatically
- **UUID**: Generate UUID in application code
- **Custom Format**: Generate custom ID pattern (AST-000001, etc.)

#### D. Employment Type Enum Validation
- **Problem**: `"invalid input value for enum employment_type: \"NA\""`
- **Solution**: 
  - Add enum value validation before database insertion
  - Provide mapping for common invalid values (NA → null, N/A → null, etc.)
  - Show enum options in field mapping interface
  - Allow users to set default values for invalid enum entries

#### E. Date Field Parsing Issues  
- **Problem**: `"invalid input syntax for type date: \"0NaN-NaN-NaNTNaN:NaN:NaN.NaN+NaN:NaN\""`
- **Solution**:
  - Implement robust date parsing with multiple format support
  - Handle empty/null date fields gracefully
  - Add date format detection and conversion
  - Provide date format selection in mapping interface
  - Show preview of parsed dates before import

**Critical Import Flow Fix:**
```javascript
const processImportRow = async (rowData, rowIndex) => {
  try {
    // 1. Generate or handle asset_id
    const processedData = { ...rowData };
    
    // Don't include asset_id in data if it's auto-generated
    if (isAutoGeneratedField('asset_id')) {
      delete processedData.asset_id;
    } else {
      processedData.asset_id = generateAssetId();
    }
    
    // 2. Validate and transform data
    processedData.employment_type = validateEmploymentType(processedData.employment_type);
    processedData.hire_date = validateDate(processedData.hire_date);
    
    // 3. Insert record
    const result = await insertAssetRecord(processedData);
    return { success: true, assetId: result.asset_id };
    
  } catch (error) {
    return { 
      success: false, 
      error: error.message,
      row: rowIndex + 1
    };
  }
};
```

### 5. Enhanced Error Handling
- **Comprehensive error logging** with specific error codes
- **User-friendly error messages** instead of technical stack traces
- **Data validation warnings** before import execution
- **Partial import success** - continue importing valid records, log failures
- **Rollback capability** if import fails partway through
- **Progress tracking** for large imports
- **Detailed import results** showing success/failure counts with specific error reasons

### 5. Export Functionality Enhancement
- **Multiple export formats**: CSV, Excel, JSON
- **Custom field selection**: Let users choose which fields to export
- **Filter options**: Export based on asset criteria
- **Batch export**: Handle large datasets efficiently

## Implementation Requirements

### Frontend Components Needed:
1. **FileUploadComponent**: Drag-drop file upload with format validation
2. **EntityTypeSelector**: Choose between Assets, Tickets, Employees import
3. **FilePreviewComponent**: Display file contents and detected format  
4. **FieldMappingComponent**: Interactive mapping interface with primary key awareness
5. **PrimaryKeyIndicator**: Visual component showing auto-generated fields
6. **ImportProgressComponent**: Real-time import progress and results
7. **MappingTemplateComponent**: Save/load mapping configurations

### Backend API Endpoints:
1. `POST /api/import/preview` - Analyze uploaded file and detect entity type
2. `GET /api/import/schema/{entityType}` - Get database schema for entity (show which fields are auto-generated)
3. `POST /api/import/validate-mapping` - Validate field mappings and primary key handling
4. `POST /api/import/execute` - Execute the import with confirmed mappings and proper ID generation
5. `GET /api/import/templates` - Get saved mapping templates
6. `POST /api/import/templates` - Save mapping template

### Key Code Areas to Review:
- **ALL import services/controllers** (assets, tickets, employees)
- **Primary key generation logic** across all entities
- **Database schema configuration** for auto-increment/serial fields
- File parsing utilities
- API request handling in frontend
- Error handling middleware
- Database transaction management
- **Entity-specific import handlers** - ensure they all handle primary keys correctly

## Success Criteria
- ✅ No more fetch API errors
- ✅ **CRITICAL**: Asset ID generation working properly - no more null constraint violations
- ✅ Proper file format support and validation
- ✅ Interactive field mapping with preview
- ✅ User can confirm mappings before import
- ✅ Clear distinction between auto-generated and user-provided fields
- ✅ Successful record insertion with proper primary key handling
- ✅ Clear error messages and progress feedback
- ✅ Ability to save and reuse mapping templates
- ✅ Robust error handling and rollback capability

## Testing Requirements
1. Test with various file formats (CSV, Excel, JSON)
2. Test with malformed files
3. Test with large datasets (1000+ records)
4. Test mapping validation with missing required fields
5. Test import rollback on errors
6. Test saved mapping template functionality
7. **Test specific data validation scenarios:**
   - Files with "NA", "N/A", empty employment_type values
   - Files with various date formats (MM/DD/YYYY, DD/MM/YYYY, YYYY-MM-DD)
   - Files with empty or malformed date fields
   - Files with mixed valid/invalid enum values
   - Files with special characters in text fields
8. **Test primary key generation:**
   - Verify asset_id is generated automatically or properly assigned
   - Test that no records fail due to null primary key constraints
   - Verify imported records have valid, unique asset_id values
   - Test rollback when primary key generation fails

Please prioritize fixing the immediate fetch API error first, then implement the field mapping system to prevent future import issues and improve user experience.